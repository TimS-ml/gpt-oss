"""
vLLM Token Generator Module

This module provides a wrapper around vLLM's LLMEngine for efficient token generation.
vLLM is a high-throughput inference library that implements:
- PagedAttention: Memory-efficient attention mechanism that reduces GPU memory usage
- Continuous batching: Dynamically batches requests for better throughput
- Prefix caching: Reuses cached KV tensors for common prompt prefixes
- Optimized CUDA kernels: Fast attention and sampling operations

The TokenGenerator class simplifies the vLLM API for streaming token generation
with support for temperature sampling, stop tokens, and log probabilities.
"""

from vllm import LLMEngine, EngineArgs, SamplingParams, TokensPrompt


class TokenGenerator:
    """
    A simplified interface for streaming token generation using vLLM's inference engine.

    This class wraps vLLM's LLMEngine to provide an easy-to-use generator-based API
    for language model inference. It handles request management, sampling parameters,
    and iterative decoding with automatic batching and memory optimization.

    Attributes:
        engine (LLMEngine): The underlying vLLM inference engine with PagedAttention
        request_id (int): Counter for generating unique request IDs
    """
    def __init__(self, model_path: str, tensor_parallel_size: int = 1):
        """
        Initialize the TokenGenerator with a language model.

        Args:
            model_path (str): Path to the model checkpoint or HuggingFace model ID.
                            vLLM will automatically load model weights and configuration.
            tensor_parallel_size (int, optional): Number of GPUs to use for tensor parallelism.
                                                 Splits model layers across multiple GPUs for larger models.
                                                 Defaults to 1 (single GPU).

        Note:
            The engine is initialized with vLLM's default settings which include:
            - Automatic GPU memory allocation with PagedAttention
            - KV cache prefix sharing for repeated prompts
            - Continuous batching for efficient multi-request processing
        """
        # Configure engine arguments for vLLM initialization
        args = EngineArgs(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
        )
        # Create the LLM engine with PagedAttention and optimized kernels
        self.engine = LLMEngine.from_engine_args(args)
        # Initialize request counter for unique request identification
        self.request_id = 0

    def generate(self,
                 prompt_tokens: list[int],
                 stop_tokens: list[int] | None = None,
                 temperature: float = 1.0,
                 max_tokens: int = 0,
                 return_logprobs: bool = False):
        """
        Generate tokens from a prompt using vLLM's streaming inference.

        This method creates a generation request and yields tokens one at a time as they
        are generated by the model. It leverages vLLM's PagedAttention for memory-efficient
        inference and continuous batching for high throughput when multiple requests are pending.

        Args:
            prompt_tokens (list[int]): List of token IDs representing the input prompt.
            stop_tokens (list[int] | None, optional): List of token IDs that should stop generation
                                                     when encountered. Defaults to None.
            temperature (float, optional): Sampling temperature for randomness control.
                                          Higher values (e.g., 1.0) increase randomness,
                                          lower values (e.g., 0.1) make output more deterministic.
                                          Defaults to 1.0.
            max_tokens (int, optional): Maximum number of tokens to generate.
                                       If 0, generates until stop token or EOS. Defaults to 0.
            return_logprobs (bool, optional): If True, yield (token_id, logprob) tuples.
                                             If False, yield only token_id. Defaults to False.

        Yields:
            int or tuple: If return_logprobs is False, yields token IDs as integers.
                         If return_logprobs is True, yields (token_id, logprob) tuples
                         where logprob is the log probability of the generated token.

        Note:
            - vLLM uses PagedAttention to efficiently manage KV cache memory
            - Prefix caching automatically reuses computation for repeated prompt prefixes
            - The engine may batch this request with other concurrent requests for efficiency
        """
        # Convert max_tokens=0 to None for unlimited generation
        if max_tokens == 0:
            max_tokens = None

        # Generate unique request ID for tracking this generation request
        request_id = str(self.request_id)
        self.request_id += 1

        # Configure sampling parameters for this generation request
        # logprobs=0 means return log probabilities for the sampled token only
        sampling_params = SamplingParams(temperature=temperature,
                                         max_tokens=max_tokens,
                                         stop_token_ids=stop_tokens,
                                         logprobs=0 if return_logprobs else None)

        # Create a prompt object from token IDs (supports vLLM's prefix caching)
        prompt = TokensPrompt(prompt_token_ids=prompt_tokens)

        # Add request to vLLM engine's request queue
        # The engine will process it with PagedAttention and continuous batching
        self.engine.add_request(request_id, prompt, sampling_params)

        # Track previously generated tokens to identify new tokens in each step
        last_token_id = []

        # Iteratively step through generation until complete
        while self.engine.has_unfinished_requests():
            # Execute one decoding step (may process multiple batched requests)
            step_outputs = self.engine.step()

            # Extract output for our request (first in batch for single request)
            output = step_outputs[0].outputs[0]
            token_ids = output.token_ids
            logprobs_list = output.logprobs if hasattr(output, "logprobs") else None

            # Identify newly generated tokens since last step
            new_token_ids = token_ids[len(last_token_id):]
            new_logprobs = logprobs_list[len(last_token_id):] if logprobs_list is not None else [None] * len(new_token_ids)

            # Yield each newly generated token with optional log probabilities
            for token_id, logprobs in zip(new_token_ids, new_logprobs):
                last_token_id.append(token_id)

                if return_logprobs:
                    # Extract log probability for the selected token
                    logprob_val = None
                    if logprobs is not None and token_id in logprobs:
                        logprob_val = logprobs[token_id].logprob
                    yield (token_id, logprob_val)
                else:
                    # Yield only the token ID
                    yield token_id

                # Early exit if a stop token is encountered
                if stop_tokens is not None and token_id in stop_tokens:
                    break
