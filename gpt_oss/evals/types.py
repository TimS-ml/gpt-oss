"""
Core Type Definitions for Evaluation Framework

This module defines the fundamental data structures used throughout the evaluation
framework. These types provide a consistent interface for:
- Representing conversations (Messages and MessageLists)
- Sampling from models (SamplerBase and SamplerResponse)
- Storing evaluation results (EvalResult and SingleEvalResult)
- Implementing new evaluations (Eval base class)

Type Hierarchy:
- Message/MessageList: Raw conversation data
- SamplerResponse: Output from model inference
- SingleEvalResult: Result from one example
- EvalResult: Aggregated results from multiple examples
- SamplerBase: Interface for model inference
- Eval: Interface for evaluation implementations
"""

from dataclasses import dataclass, field
from typing import Any, Literal, overload

# Message represents a single turn in a conversation
# Keys typically include 'role' (user/assistant/system) and 'content' (the text)
Message = dict[str, Any]

# MessageList represents a full conversation as a sequence of messages
MessageList = list[Message]


@dataclass
class SamplerResponse:
    """
    Response from a model sampler after inference.

    This class encapsulates everything returned by a model when it generates
    a response to a conversation.

    Attributes:
        response_text: The actual text generated by the model
        actual_queried_message_list: The complete message list that was sent to the model,
            including any system messages or reasoning steps that were added
        response_metadata: Additional metadata about the response, such as:
            - usage: Token counts (input_tokens, output_tokens, etc.)
            - model information
            - API-specific data
    """
    response_text: str
    actual_queried_message_list: MessageList
    response_metadata: dict[str, Any]


class SamplerBase:
    """
    Base class for model samplers.

    A sampler is responsible for interfacing with a language model to generate
    responses. This can be used for:
    1. The model being evaluated
    2. A grader model used to assess responses (e.g., GPT-4.1 for HealthBench)

    Implementations include:
    - ChatCompletionsSampler: Uses OpenAI Chat Completions API
    - ResponsesSampler: Uses OpenAI Responses API (for reasoning models)

    To implement a new sampler:
    1. Subclass SamplerBase
    2. Implement __call__ to handle message_list -> SamplerResponse
    3. Optionally implement _pack_message for message formatting
    """

    def __call__(
        self,
        message_list: MessageList,
    ) -> SamplerResponse:
        """
        Generate a response to the given conversation.

        Args:
            message_list: The conversation history to respond to

        Returns:
            SamplerResponse containing the generated text and metadata
        """
        raise NotImplementedError


@dataclass
class EvalResult:
    """
    Aggregated results from running an evaluation across multiple examples.

    This represents the final output of an evaluation, combining results
    from all individual examples into summary statistics.

    Attributes:
        score: The primary evaluation metric (e.g., accuracy, pass@1)
            - For GPQA: accuracy on multiple-choice questions
            - For AIME: exact match accuracy on integer answers
            - For HealthBench: rubric-based score [0, 1]
        metrics: Additional metrics specific to the evaluation
            - May include standard deviations, bootstrap estimates, subscores
            - Example: {"accuracy:std": 0.05, "accuracy:n_samples": 100}
        htmls: List of HTML snippets for each example, used to generate reports
            - Each HTML shows the prompt, response, and evaluation details
        convos: Complete conversation history for each example
            - Useful for post-hoc analysis and debugging
        metadata: Evaluation-specific metadata
            - Example: rubric scores, physician completions, etc.
    """

    score: float | None  # Primary metric for this evaluation
    metrics: dict[str, float] | None  # Additional metrics and statistics
    htmls: list[str]  # HTML representations of each example
    convos: list[MessageList]  # Full conversation histories
    metadata: dict[str, Any] | None  # Evaluation-specific data


@dataclass
class SingleEvalResult:
    """
    Result from evaluating a single example.

    This represents the outcome of running one test case through the evaluation
    pipeline. Multiple SingleEvalResults are aggregated into an EvalResult.

    Attributes:
        score: The score for this individual example (0.0 or 1.0 for binary tasks)
        metrics: Example-level metrics (e.g., response length, tokens used)
        html: HTML representation of this example for the report
        convo: The complete conversation for this example
        example_level_metadata: Additional data specific to this example
            - For HealthBench: individual rubric item scores
            - For other evals: extracted answers, correct answers, etc.
    """

    score: float | None
    metrics: dict[str, float] = field(default_factory=dict)
    html: str | None = None
    convo: MessageList | None = None
    example_level_metadata: dict[str, Any] | None = (
        None  # Evaluation-specific metadata for this example
    )


class Eval:
    """
    Base class for implementing evaluations.

    An Eval represents a specific benchmark or test suite. To implement a new
    evaluation:

    1. Subclass Eval
    2. Implement __init__ to load the dataset and configure the evaluation
    3. Implement __call__ to:
       a. Run the sampler on each example
       b. Extract and grade answers
       c. Return an EvalResult with aggregated statistics

    Evaluation Flow:
    1. User calls eval_instance(sampler)
    2. Eval iterates over its dataset
    3. For each example:
       - Format the prompt
       - Call sampler(prompt) to get response
       - Extract the answer from the response
       - Compare to ground truth and compute score
       - Create SingleEvalResult
    4. Aggregate all SingleEvalResults into final EvalResult

    Examples:
    - GPQAEval: Multiple-choice science questions
    - AIME25Eval: Math problems with integer answers
    - HealthBenchEval: Medical Q&A with rubric-based grading
    """

    def __call__(self, sampler: SamplerBase) -> EvalResult:
        """
        Run the evaluation using the given sampler.

        Args:
            sampler: The model to evaluate (or SamplerBase for human baselines)

        Returns:
            EvalResult containing aggregated scores and metrics
        """
        raise NotImplementedError

